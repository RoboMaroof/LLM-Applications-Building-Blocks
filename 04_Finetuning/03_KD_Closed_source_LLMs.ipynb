{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBod+LDB8OMpCHwGhRiFcZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoboMaroof/LLM-Applications-Building-Blocks/blob/main/04_Finetuning/03_KD_Closed_source_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/clone-the-abilities-of-powerful-llms-into-small-local-models-using-knowledge-distillation-12e954d256c2\n",
        "\n",
        "https://github.com/CVxTz/distill-llm\n",
        "\n",
        "https://github.com/CVxTz/knowledge_distillation\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html\n",
        "\n",
        "https://towardsdatascience.com/build-powerful-lightweight-models-using-knowledge-distillation-618f69b569d9"
      ],
      "metadata": {
        "id": "VoRwzraB5oge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distillation Workflow"
      ],
      "metadata": {
        "id": "mveDAQAK6Oew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Acquire unlabeled in-domain data.\n",
        "2. raft a prompt to extract pseudo-labels from the teacher model by leveraging Anyscale’s API.\n",
        "3. Fine-tune the student model on these pseudo labels using LoRa + Peft."
      ],
      "metadata": {
        "id": "a7LV4BBt55Jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "3bFx6BVv6SMl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtFeMXfd5mNe"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "data = load_dataset(\"juancavallotti/multilingual-gec\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the labels for evaluation and not for training."
      ],
      "metadata": {
        "id": "yKMQZxc36W3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teacher Model"
      ],
      "metadata": {
        "id": "Tz9HB_Oh6dpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Teacher Model: LLama 2–70B**\n",
        "- Produce the pseudo-labels that will be used for the training.\n",
        "- Powerful LLM hosted as pay-per-use API on cloud.\n",
        "\n",
        "Options:\n",
        "- OpenAI\n",
        "- Anthropic\n",
        "- AnyScale (used here)\n",
        "\n",
        "Generation of pseudo-labels for around 5000 samples costs 1.2 dollars through Anyscale."
      ],
      "metadata": {
        "id": "zeUz7mtGCISk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "BASE_URL = \"https://api.endpoints.anyscale.com/v1\"\n",
        "BASE_MODEL = \"meta-llama/Llama-2-70b-chat-hf\"\n",
        "\n",
        "BASE_CLIENT = OpenAI(base_url=BASE_URL, api_key=API_KEY)\n",
        "\n",
        "def process_call(prompt):\n",
        "\n",
        "    completion = BASE_CLIENT.completions.create(\n",
        "        model=BASE_MODEL,\n",
        "        prompt=prompt,\n",
        "        max_tokens=100,\n",
        "        temperature=0,\n",
        "    )\n",
        "    result = completion.model_dump()\n",
        "\n",
        "    return result[\"choices\"][0][\"text\"].strip()"
      ],
      "metadata": {
        "id": "vsKPPhmR6bY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt"
      ],
      "metadata": {
        "id": "gcQLjtO4DI_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "<s>[INST]\n",
        "Your role is to correct all grammatical errors in the input text. Only answer with the corrected text and nothing else.\n",
        "\n",
        "Text: Il est très importante de parler une langue étrangère.\n",
        "[/INST]\n",
        "Output: Il est très important de parler une langue étrangère.</s>\n",
        "[INST]\n",
        "Text: Nadie dise ezo.\n",
        "[/INST]\n",
        "Output: Nadie dice eso.</s>\n",
        "[INST]\n",
        "Text: What is your favorite part of being a member of SWE RMS?\n",
        "[/INST]\n",
        "Output: What is your favorite part of being a member of SWE RMS?</s>\n",
        "[INST]\n",
        "Text: I looked, at the schedule.\n",
        "[/INST]\n",
        "Output: I looked at the schedule.</s>\n",
        "[INST]\n",
        "Text: $text\n",
        "[/INST]\n",
        "Output:"
      ],
      "metadata": {
        "id": "CANlU3PgDKSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Student Model"
      ],
      "metadata": {
        "id": "TEraqu1UDQLP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Student Model: Tiny-LLama-1B**\n",
        "\n",
        "- “Train” on the grammar correction task using the pseudo-labels from the teacher model.\n",
        "- Despite its smaller scale highly efficient.\n",
        "- Can run on consumer GPUs with just a few gigabytes of memory.\n",
        "- Can be run as a HuggingFace Pipeline.\n",
        "\n",
        "BitsAndBytes used here for GPU quantization, which reduces the memory requirements of running LLMs."
      ],
      "metadata": {
        "id": "agd1uNLdDT5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline,\n",
        ")\n",
        "\n",
        "base_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "llama_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_name, trust_remote_code=True\n",
        ")\n",
        "llama_tokenizer.padding_side = \"right\"\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "# Model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=quant_config,\n",
        "    device_map={\"\": 0},\n",
        ")\n",
        "\n",
        "text_gen = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=llama_tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    do_sample=False,\n",
        "    return_full_text=False,\n",
        ")"
      ],
      "metadata": {
        "id": "9rn6SBAjDS0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_gen(\"Hello ! Who are you ?\"))"
      ],
      "metadata": {
        "id": "gx3V25L-E-O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA Finetuning"
      ],
      "metadata": {
        "id": "aADnP6vgGACI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from trl import SFTTrainer\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    .\n",
        "    .\n",
        "    .\n",
        "    .\n",
        "    peft_parameters = LoraConfig(\n",
        "        lora_alpha=8,\n",
        "        lora_dropout=0.1,\n",
        "        r=8,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        # target_modules=target_modules,\n",
        "    )\n",
        "\n",
        "    base_model = prepare_model_for_kbit_training(base_model)\n",
        "    base_model = get_peft_model(base_model, peft_parameters)\n",
        "\n",
        "    # Training Params\n",
        "    train_params = TrainingArguments(\n",
        "        output_dir=str(BASE_PATH / \"results_modified\"),\n",
        "        num_train_epochs=EPOCHS,\n",
        "        per_device_train_batch_size=8,\n",
        "        gradient_accumulation_steps=1,\n",
        "        optim=\"paged_adamw_32bit\",\n",
        "        save_steps=len(training_data) // 10,\n",
        "        logging_steps=len(training_data) // 100,\n",
        "        learning_rate=2e-4,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_steps=100,\n",
        "        weight_decay=0.05,\n",
        "        fp16=True,\n",
        "        max_steps=-1,\n",
        "        group_by_length=False,\n",
        "        max_grad_norm=0.3,\n",
        "    )\n",
        "    # Trainer\n",
        "    fine_tuning = SFTTrainer(\n",
        "        model=base_model,\n",
        "        train_dataset=training_data,\n",
        "        data_collator=collator,\n",
        "        peft_config=peft_parameters,\n",
        "        dataset_text_field=\"Why is this mandatory ?\",\n",
        "        tokenizer=llama_tokenizer,\n",
        "        args=train_params,\n",
        "        max_seq_length=llama_tokenizer.model_max_length,\n",
        "    )\n",
        "\n",
        "    print(fine_tuning.model.print_trainable_parameters())\n",
        "    # Training\n",
        "    fine_tuning.train()"
      ],
      "metadata": {
        "id": "qxfC3t_VGCZu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}